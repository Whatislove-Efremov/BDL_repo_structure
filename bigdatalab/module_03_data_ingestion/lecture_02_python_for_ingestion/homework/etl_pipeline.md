# Задание: Загрузка CSV-файлов в Bronze слой EcoMarket

## Цель работы

Разработать Python-модуль для загрузки исходных CSV-файлов в реляционную базу данных **EcoMarket (Bronze Layer)**.

В рамках работы необходимо:
- спроектировать структуру таблиц первичного слоя хранения данных;
- реализовать загрузку данных из CSV в базу данных;
- закрепить навыки работы с библиотеками `pandas` и `SQLAlchemy`;
- обеспечить корректную обработку ошибок при загрузке.

После выполнения работы база данных должна содержать полный набор исходных данных в структурированном виде.

---

## Подготовка данных

Перед выполнением задания необходимо:

- скачать CSV-файлы датасета из Google Drive (ссылка предоставляется отдельно);
- поместить все CSV-файлы в папку `data` внутри директории с заданием.

Ожидаемая структура:

```
homework/
├── data/
│   ├── countries.csv
│   ├── cities.csv
│   ├── categories.csv
│   ├── products.csv
│   ├── shops.csv
│   ├── employees.csv
│   ├── customers.csv
│   └── sales.csv
```


⚠️ Убедитесь, что все файлы находятся в папке `data` **до запуска ETL-скрипта**.

---

## Архитектурный контекст

В последующих разделах курса вы познакомитесь с концепцией **медальонной архитектуры (Bronze / Silver / Gold)** и построением Data Warehouse.

На данном этапе:
- требуется создать первичный слой хранения данных;
- не требуется выполнять очистку, дедупликацию или аналитические преобразования;
- не требуется проектировать схему «звезда» или разделять факты и измерения.

Текущая задача — организовать корректную загрузку сырых исходных данных (`csv`) в базу данных.

---

## Требования к структуре базы данных

Необходимо создать таблицы Bronze слоя для хранения данных из следующих CSV-файлов:

- `countries.csv`
- `cities.csv`
- `categories.csv`
- `products.csv`
- `shops.csv`
- `employees.csv`
- `customers.csv`
- `sales.csv`

### Важно

- Все таблицы Bronze слоя должны быть **в схеме `bronze`**.
- Все таблицы должны иметь префикс `bronze_`.

Примеры:
- `bronze_countries`
- `bronze_products`
- `bronze_sales`

### Структурные требования

- Каждая таблица должна иметь **Primary Key (PK)**.
- Должны быть настроены **Foreign Key (FK)** согласно связям датасета.
- Типы данных должны соответствовать содержимому (`numeric`, `varchar`, `integer` и т.д.).
- Даты могут храниться как `varchar`  
  (очистка и приведение типов будут выполняться позже).

---

## Технологический стек

- Python 3.x
- pandas
- SQLAlchemy
- PostgreSQL

---

## Этапы выполнения работы

### Этап 1. Настройка соединения

- Создать Python-скрипт `etl_pipeline.py`.
- Импортировать необходимые библиотеки.
- Настроить строку подключения к базе данных.
- Инициализировать объект `Engine` через `create_engine`.
- Реализовать базовую обработку ошибок подключения (`try-except`).

---

### Этап 2. Загрузка таблиц-справочников

- Считать CSV-файл через `pandas.read_csv()`.
- При необходимости привести названия столбцов к формату БД.
- Загрузить данные в таблицу через `DataFrame.to_sql()`.

Рекомендуемый порядок загрузки (для соблюдения ссылочной целостности):

1. `countries`
2. `cities`
3. `categories`
4. `products`
5. `shops`
6. `employees`
7. `customers`

---

### Этап 3. Загрузка таблицы продаж

- Считать `sales.csv` через pandas.
- Отключить выгрузку индекса (`index=False`).
- Загрузить данные в таблицу `bronze_sales`.
- Учитывать большой объём файла (6+ млн строк) — использовать только векторную загрузку.

---

## Технические требования

### Запрещено

- использовать циклы `for` / `while` для построчной вставки данных;
- выполнять `INSERT` вручную для каждой строки.

### Обязательно

- использовать `to_sql()` для массовой загрузки;
- установить `if_exists='append'`;
- обернуть ключевые операции в `try-except`;
- выводить понятные сообщения об ошибках в консоль.

### Дополнительно рекомендуется

- оформлять код в виде функций;
- добавлять `docstring` к основным функциям;
- писать читаемый и структурированный код.

---

## Особенности датасета

В датасете присутствуют:
- текстовые даты (`varchar`);
- некорректные даты в `employees`;
- пустые или «битые» timestamps в `sales`;
- дубликаты сотрудников;
- аномалии продаж.

На данном этапе:
- допускается загрузка данных без глубокой очистки;
- базовая обработка ошибок приветствуется;
- полноценная проверка качества данных будет рассмотрена в теме **DQ for DE module**.

---

## Работа с GitHub

При загрузке проекта в GitHub **не рекомендуется коммитить CSV-файлы из папки `data`**.

Причины:
- файлы имеют большой размер;
- данные предоставляются отдельно;
- в репозитории должен храниться только код.

Рекомендуется добавить папку `data` или файлы `*.csv` в `.gitignore`.



---

## Критерии приёмки

Работа считается выполненной, если:
- скрипт запускается без синтаксических ошибок;
- таблицы Bronze слоя созданы;
- данные из всех CSV-файлов загружены в соответствующие таблицы;
- количество строк в таблицах соответствует количеству строк в исходных файлах;
- внешние ключи настроены корректно;
- загрузка выполняется без использования циклов.

---

## Ожидаемый результат

Количество строк в базе данных соответствует количеству строк в CSV-файлах.

